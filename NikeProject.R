## Nike Project

## Given the sales of Nike Quarterly
#Robertashas collected quarterly data on Nike’s revenue for the fiscal years 1999 through 2008; 
#for instance, data for fiscal year 1999 refers to the time period from June 1, 1998 through May 31, 1999. 
#Validation set = 2009 data.

#Given the Data set , The Quarters are formed in this way 
# Q1 -- June to Aug
# Q2 -- Sep to Dec
# Q3 -- Dec to Feb
# Q4 -- Mar - May 

# Load the data into R 
data <- read.csv("Revenuenike.csv",header = TRUE, sep = ",")

# The initial data was not given in this way. I have modified and made some changes so that it is easy for computations

#There are 5 crucial methods of dealing this 
# 1. Linear Regression 
# 2. Smooting Method (Data-Driven Method)
# 3. Classical Time Series Decomposition 
# 4. ARIMA Models 
# 5. Ensemble methods (Combaining 2-3 models)


## Data Exploration 
library(ggplot2)
str(data)

# Plotting the data
ggplot(data,aes(x=Time,y=Revenue))+
  geom_line()+
  geom_point()

# Findings.
# 1. There is a Trend(Increasing) in given Data. The Sales are increasing overall along the data period
# 2. There is a seasonality in the data.
# 3. The amplitude of seasonality is increasing with time.


## Model Building 
# 1. Linear Regression 
# Creating Dummies for the Quarters.
library(dplyr)
data <- mutate(data, Q1 = ifelse(Quarter == 1,1,0))
data <- mutate(data, Q2 = ifelse(Quarter == 2,1,0))
data <- mutate(data, Q3 = ifelse(Quarter == 3,1,0))
data <- mutate(data, Q4 = ifelse(Quarter == 4,1,0))

#Dividing the data into training and validation test
data_train <- data[1:40,]
data_validation <- data[41:44,]

# Building a linear Regression 
LR1 <- lm(Revenue~. -Quarter, data = data_train)
summary(LR1)

plot(LR1$residuals,type ="l")
plot(data_train$Revenue,type="l")
lines(LR1$fitted,lwd=2,col = "red")

# 1. There is Cyclicity in the Data clearly.
# 2. The increasing amplitude is not captured appropriately.

LR2 <- lm(log(Revenue)~.-Q1-Q4-Quarter, data = data)
summary(LR2)

plot(LR2$residuals,type ="l")
plot(log(data_train$Revenue),type="l")
lines(LR2$fitted,lwd=2,col = "red")



##There is a cyclicity Since we haven't captured the Residuals
library(forecast)
library(tseries)
Acf(LR2$residuals,lag.max = 15,main="")

# Forecasting the residuals using the ARIMA Model 
train_residuals <- Arima(LR2$residuals,order = c(2,0,0))
pred_residuals <- forecast(train_residuals,h= 1*4)


# MSE 
predicted.Arima.residuals <- pred_residuals$fitted
predicted.linear <- predict(LR2,data)
Total_predictions <- exp(predicted.Arima.residuals+predicted.linear)

# Plots 
plot(data$Revenue,type="l")
lines(Total_predictions,lwd=2,col = "red")


# Training MSE 
Train_MSE <- mean((Total_predictions[1:40]-data$Revenue[1:40])^2)
## Train_MSE == 12989.05
Train_RMSE = (Train_MSE)^0.5 ## 113.9695

# Testing MSE 
Test_MSE <- mean((Total_predictions[41:44]-data$Revenue[41:44])^2)
## Test_MSE == 156187.7
Test_RMSE=(Test_MSE)^0.5 ## 395.2059


## Comparing the Values 
data$Revenue[41:44]
Total_predictions[41:44]
####################################################################################

## Smooting Methods
library(forecast)


train.ts <- ts(data$Revenue[1:40],start=c(1999,1),frequency = 4)
valid.ts <- ts(data$Revenue[41:44],start=c(2009,1),frequency = 4)

## Checking for stationrity

adf.test(train.ts)
kpss.test(train.ts)
# Above both clearly shows that the data is not stationary

# Display of the graph
tsdisplay(train.ts)


#Model1
#Automatic model generated by the R 
ses <- ets(train.ts,model="ZZZ")
summary(ses)
ses.pred <- forecast(ses,h=1*4)


plot(ses.pred,type="l")
lines(ses.pred$fitted,lwd=2,col = "red")

Testing_RMSE = (mean((ses.pred$mean-valid.ts)^2))^0.5
#Training RMSE == 244.1498
#Testing RMSE == 725.7154 


## Model2
#Automatic model generated by the R 
ses2 <- ets(train.ts,model="MMM",restrict = FALSE)
summary(ses2)
ses2.pred <- forecast(ses2,h=1*4)


plot(ses2.pred,type="l")
lines(ses2.pred$fitted,lwd=2,col = "red")

Testing_RMSE = (mean((ses2.pred$mean-valid.ts)^2))^0.5
#Training RMSE == 98.69906
#Testing RMSE == 704.7154 


# Holts - Winter Method 
hw <- HoltWinters(train.ts)
plot(hw)

forecast <- predict(hw, n.ahead = 4, prediction.interval = T, level = 0.95)
plot(hw, forecast)

forecast
Testing_RMSE = (mean((forecast-valid.ts)^2))^0.5

# Test_RMSE = 775.7122
forecast
########################################################################################


## Plotting the Data
library(ggplot2)
ggplot(data,aes(x=Time, y = Revenue))+
  geom_line()

# Asumptions 
# 1. We see that there is a seasonal Variation 
# 2. We see that the Seasonality amplitude is increasing with time 
# 3. We see that there is a trend in the data 

#22. Is the data stationary? How do you know? Is there a way to make nonstationary data stationary? 
#How? Apply these ideas to Nike’s revenue data.
#Ans
tsdisplay(data$Revenue)
# The data is not stationary as there is a trend in the plot shown.

# Yes there is a way to make non-stationary data stationary
# Removing the trend 
tsdisplay(diff(data$Revenue,4))
# Removing the trend and seasonality
tsdisplay(diff(diff(data$Revenue),4))

#Model -1
# Automatic
M=auto.arima(train.ts)
M

#ARIMA(1,1,0)(0,1,0)[4]
#AIC - 422.23
# auto arima selects the best model using smallest AIC, AICC or BIC value.
# AIC, AICC or BIC = penalized prediction error. More complex models are penalized more


# Make prediction

MF=forecast(M,h=1*4) # predict next three years values
MF

plot(MF)
lines(fitted(M),col=2)

RMSE = accuracy(MF,valid.ts)
#RMSE(test) = 702.43821
#RMSE(train) = 88.65624

#Model -2
# Offing the Stepwise
M1=auto.arima(train.ts,stepwise = F,approximation = F)
M1

# ARIMA (0,1,2)(0,1,0)[4]
# AIC = 421.78

MF1=forecast(M1,h=1*4) # predict next three years values
MF1


RMSE = accuracy(MF1,valid.ts)
#RMSE(testing) = 764.42
#RMSE(train) = 85.16244

# The AIC is almost the same as before 
# Determine whether a model(s) is(are) adequate!
# Dtermine dependence structure of residuals(=actual-predicted)
# model is adequate if residuals contain no pattern, i.e. residuals are independent

tsdisplay(residuals(M))
tsdisplay(residuals(M1))
# Both the Models are the same 

#Model - 3 
# Manual Based on ACF and PACF 
tsdisplay(train.ts)
# LAG1 is highly correlated and Beta1 is the only siginificant with Beta2 somewhat significant 
M2 = Arima(train.ts,order=c(1,1,1))
M2

# AIC = 562.24
MF2=forecast(M2,h=1*4) # predict next three years values
MF2


RMSE = accuracy(MF2,valid.ts)
#RMSE(testing) = 298.28
#RMSE(train) = 380.1368
#The AIC value is 562.24. Which is bad compared to the previous value

#Model - 4 
M3 = Arima(train.ts,order=c(2,1,2),seasonal = c(0,1,0))
M3
# The AIC is 423.79 which is a considerable improvement.

MF3=forecast(M3,h=1*4) # predict next three years values
MF3


RMSE = accuracy(MF3,valid.ts)
#RMSE(testing) = 84.942
#RMSE(train) = 749.46

############################################################################################


# Decomposition Method 

# We use the same library package "forecast" for developing the Model 

library(forecast)
train.ts <- ts(data$Revenue[1:40],start=c(1999,1),frequency = 4)
valid.ts <- ts(data$Revenue[41:44],start=c(2009,1),frequency = 4)


decom_model <- stl(train.ts, s.window = "periodic")
plot(decom_model)

# Modelling 
M1 <- stlm(train.ts,method = c("ets"),etsmodel = "ZZZ")
M1

#Plot
plot(forecast,type="l")
forecast <- predict(M1, n.ahead = 4, prediction.interval = T, level = 0.95)
plot(forecast,type = "l")

# RMSE 
RMSE = accuracy(forecast,valid.ts)


# RMSE_Train = 73.36658
# RMSE_Test = 744.81155

